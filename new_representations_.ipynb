{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4f0510",
      "metadata": {
        "id": "ca4f0510"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e2e002a",
      "metadata": {
        "id": "4e2e002a"
      },
      "outputs": [],
      "source": [
        "df_sentencias = pd.read_json('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.jsonl',lines=True)\n",
        "df_sentencias['bias'] = [1 if len(x) > 0 else 0 for x in df_sentencias['highlight']]\n",
        "df_sentencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7032c624",
      "metadata": {
        "id": "7032c624"
      },
      "outputs": [],
      "source": [
        "from genbit.genbit_metrics import GenBitMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9529a2a3",
      "metadata": {
        "id": "9529a2a3"
      },
      "outputs": [],
      "source": [
        "listi = []\n",
        "\n",
        "language_code = 'es'\n",
        "for i in tqdm(range(0,len(df_sentencias))):\n",
        "    x = df_sentencias['text'].values[i]\n",
        "    genbit_metrics_object = GenBitMetrics(language_code, context_window=3, distance_weight=0.95, percentile_cutoff=80)\n",
        "    genbit_metrics_object.add_data(x, tokenized=False)\n",
        "    metrics = {}\n",
        "    metrics['doc'] = df_sentencias['doc'].values[i]\n",
        "    metrics['page'] = df_sentencias['page'].values[i]\n",
        "    metrics['text'] = x\n",
        "    metrics['bias'] = df_sentencias['bias'].values[i]\n",
        "    metrics.update(genbit_metrics_object.get_metrics(output_statistics=True, output_word_list=False))\n",
        "    listi.append(metrics)\n",
        "\n",
        "df_microsoft = pd.DataFrame(listi)\n",
        "df_microsoft.to_pickle('df_microsoft.pickle')\n",
        "df_microsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33536e86",
      "metadata": {
        "id": "33536e86"
      },
      "outputs": [],
      "source": [
        "aa = pd.json_normalize(df_microsoft['additional_metrics'])\n",
        "aa = aa.rename(columns={x:'additional_metrics__'+x for x in aa.columns})\n",
        "\n",
        "bb = pd.json_normalize(df_microsoft['statistics'])\n",
        "bb = bb.rename(columns={x:'statistics__'+x for x in bb.columns})\n",
        "\n",
        "cc = pd.concat([df_microsoft,aa,bb],axis=1)\n",
        "cc = cc.drop(columns=['additional_metrics','statistics'])\n",
        "cc.to_pickle('df_microsoft_normalized.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81065113",
      "metadata": {
        "id": "81065113"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# load tagger\n",
        "tagger = SequenceTagger.load(\"aymurai/flair-ner-spanish-judicial\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59382b9",
      "metadata": {
        "id": "f59382b9"
      },
      "outputs": [],
      "source": [
        "listi = []\n",
        "for i in tqdm(range(0,len(df_sentencias))):\n",
        "    x = df_sentencias['text'].values[i]\n",
        "    metrics = {}\n",
        "    metrics['doc'] = df_sentencias['doc'].values[i]\n",
        "    metrics['page'] = df_sentencias['page'].values[i]\n",
        "    metrics['text'] = x\n",
        "    metrics['bias'] = df_sentencias['bias'].values[i]\n",
        "    x = Sentence(x)\n",
        "    tagger.predict(x)\n",
        "    metrics['NER'] = [entity.to_dict() for entity in x.get_spans('ner')]\n",
        "    listi.append(metrics)\n",
        "\n",
        "df_ner = pd.DataFrame(listi)\n",
        "df_ner.to_pickle('df_ner.pickle')\n",
        "df_ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be49484",
      "metadata": {
        "id": "6be49484"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "listi = []\n",
        "listi_avg = []\n",
        "for i in range(0,len(df_ner['NER'])):\n",
        "    x = df_ner['NER'].values[i]\n",
        "    metrics = {}\n",
        "    metrics['doc'] = df_sentencias['doc'].values[i]\n",
        "    metrics['page'] = df_sentencias['page'].values[i]\n",
        "    metrics['text'] = df_sentencias['text'].values[i]\n",
        "    metrics['bias'] = df_sentencias['bias'].values[i]\n",
        "    metrics_avg = dict(metrics)\n",
        "    confidences = defaultdict(list)\n",
        "    for y in x:\n",
        "        for z in y['labels']:\n",
        "            confidences[z['value']].append(z['confidence'])\n",
        "        for k,v in confidences.items():\n",
        "            metrics_avg[k] = sum(v) / len(v)\n",
        "            metrics[k] = len(v)\n",
        "    listi.append(metrics)\n",
        "    listi_avg.append(metrics_avg)\n",
        "\n",
        "dd_avg = pd.DataFrame(listi_avg)\n",
        "dd_avg.to_pickle('df_NER_normalized_avg.pickle')\n",
        "dd_avg.to_csv('df_NER_normalized_avg.csv',index=False)\n",
        "\n",
        "dd = pd.DataFrame(listi)\n",
        "dd.to_pickle('df_NER_normalized.pickle')\n",
        "dd.to_csv('df_NER_normalized.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abea7f2",
      "metadata": {
        "id": "3abea7f2"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", do_lower_case=False)\n",
        "model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed6014a",
      "metadata": {
        "id": "fed6014a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import IntTensor\n",
        "listi = []\n",
        "listi_embeddings = []\n",
        "for i in tqdm(range(0,len(df_sentencias))):\n",
        "    x = df_sentencias['text'].values[i]\n",
        "    metrics = {}\n",
        "    metrics['doc'] = df_sentencias['doc'].values[i]\n",
        "    metrics['page'] = df_sentencias['page'].values[i]\n",
        "    metrics['text'] = x\n",
        "    metrics['bias'] = df_sentencias['bias'].values[i]\n",
        "    tokens = tokenizer(x, return_tensors='pt')\n",
        "    if len(tokens[\"input_ids\"][-1]) > 512:\n",
        "        # https://arxiv.org/pdf/1905.05583.pdf\n",
        "        tokens_ = {}\n",
        "        tokens_['input_ids'] = Tensor([tokens['input_ids'][-1][:128].tolist() + tokens['input_ids'][-1][-384:].tolist()]).int()\n",
        "        tokens_['token_type_ids'] = IntTensor([tokens['token_type_ids'][-1][:128].tolist() + tokens['token_type_ids'][-1][-384:].tolist()])\n",
        "        tokens_['attention_mask'] = Tensor([tokens['attention_mask'][-1][:128].tolist() + tokens['attention_mask'][-1][-384:].tolist()]).to(torch.int64)\n",
        "        tokens = transformers.BatchEncoding(tokens_)\n",
        "\n",
        "    print(len(tokens['input_ids'][-1]))\n",
        "    outputs = model(**tokens)\n",
        "    outputs = outputs['last_hidden_state'][-1][-1]\n",
        "\n",
        "    listi.append(metrics)\n",
        "    listi_embeddings.append(outputs.tolist())\n",
        "\n",
        "df_bert = pd.concat([pd.DataFrame(listi),pd.DataFrame(listi_embeddings)],axis=1)\n",
        "df_bert.to_pickle('df_bert.pickle')\n",
        "df_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac05880",
      "metadata": {
        "id": "0ac05880"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "\n",
        "model = SentenceTransformer('espejelomar/sentece-embeddings-BETO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e168354",
      "metadata": {
        "id": "2e168354"
      },
      "outputs": [],
      "source": [
        "listi = []\n",
        "listi_embeddings = []\n",
        "for i in tqdm(range(0,len(df_sentencias))):\n",
        "    x = df_sentencias['text'].values[i]\n",
        "    metrics = {}\n",
        "    metrics['doc'] = df_sentencias['doc'].values[i]\n",
        "    metrics['page'] = df_sentencias['page'].values[i]\n",
        "    metrics['text'] = x\n",
        "    metrics['bias'] = df_sentencias['bias'].values[i]\n",
        "    embeddings = model.encode(x)\n",
        "    listi.append(metrics)\n",
        "    listi_embeddings.append(embeddings)\n",
        "\n",
        "df_st = pd.concat([pd.DataFrame(listi),pd.DataFrame(listi_embeddings)],axis=1)\n",
        "df_st.to_pickle('df_st.pickle')\n",
        "df_st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b9c2010",
      "metadata": {
        "id": "3b9c2010"
      },
      "outputs": [],
      "source": [
        "df_sentencias = df_sentencias[~df_sentencias['doc'].str.startswith('xxxxxxxxxxxxxxxxxxxxxxxxxxxx')]\n",
        "df_sentencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbebc7a2",
      "metadata": {
        "id": "bbebc7a2"
      },
      "outputs": [],
      "source": [
        "aa = df_sentencias.groupby('doc')[['bias']].sum()\n",
        "aa = set(aa[aa['bias'] > 0].index)\n",
        "df_sentencias_with = df_sentencias[df_sentencias['doc'].isin(aa)]\n",
        "df_sentencias_with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8355621",
      "metadata": {
        "id": "f8355621"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import StratifiedGroupKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06dbfe5",
      "metadata": {
        "id": "f06dbfe5"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "def get_splits(df_sentencias, name, n_splits=5, ss=None):\n",
        "    listi_ids = []\n",
        "    listi_rows = []\n",
        "    print(name)\n",
        "    for partition in tqdm(ss.split(df_sentencias,y=df_sentencias['bias'])):\n",
        "#         print(partition[0])\n",
        "#         print(partition[1])\n",
        "#         print('------------')\n",
        "        listi_ids.append((list(df_sentencias.iloc[partition[0]].index),list(df_sentencias.iloc[partition[1]].index)))\n",
        "        listi_rows.append(([(x[1][3],x[1][0],x[1][1]) for x in df_sentencias[df_sentencias.index.isin(listi_ids[-1][0])].iterrows()],[(x[1][3],x[1][0],x[1][1]) for x in df_sentencias[df_sentencias.index.isin(listi_ids[-1][1])].iterrows()]))\n",
        "\n",
        "    with open(f'split_{name}_{n_splits}_ids.pickle','wb') as file:\n",
        "        pickle.dump(listi_ids,file)\n",
        "\n",
        "    with open(f'split_{name}_{n_splits}_rows.pickle','wb') as file:\n",
        "        pickle.dump(listi_rows,file)\n",
        "    return listi_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1965f55",
      "metadata": {
        "id": "c1965f55"
      },
      "outputs": [],
      "source": [
        "ss = KFold(n_splits=5,random_state=None)\n",
        "name = 'KFold_alldocs'\n",
        "get_splits(df_sentencias, name, n_splits=5, ss=ss)\n",
        "\n",
        "name = 'StratifiedKFold_alldocs'\n",
        "ss = StratifiedKFold(n_splits=5,random_state=None)\n",
        "get_splits(df_sentencias, name, n_splits=5, ss=ss)\n",
        "\n",
        "ss = KFold(n_splits=5,random_state=None)\n",
        "name = 'KFold_with'\n",
        "get_splits(df_sentencias_with, name, n_splits=5, ss=ss)\n",
        "\n",
        "name = 'StratifiedKFold_with'\n",
        "ss = StratifiedKFold(n_splits=5,random_state=None)\n",
        "listi = get_splits(df_sentencias_with, name, n_splits=5, ss=ss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09eb663",
      "metadata": {
        "id": "e09eb663"
      },
      "outputs": [],
      "source": [
        "i = 5\n",
        "df_sentencias_with[df_sentencias_with.index.isin(listi[i][1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "854a50a8",
      "metadata": {
        "id": "854a50a8"
      },
      "outputs": [],
      "source": [
        "def get_group_splits(df_sentencias, name, n_splits=5, ss=None):\n",
        "    listi_ids = []\n",
        "    listi_rows = []\n",
        "    print(name)\n",
        "    for train_index, test_index in ss.split(df_sentencias.index, df_sentencias['bias'],groups=df_sentencias['doc']):\n",
        "        listi_ids.append((list(df_sentencias.iloc[train_index].index),list(df_sentencias.iloc[test_index].index)))\n",
        "        listi_rows.append(([(x[1][3],x[1][0],x[1][1]) for x in df_sentencias[df_sentencias.index.isin(listi_ids[-1][0])].iterrows()],[(x[1][3],x[1][0],x[1][1]) for x in df_sentencias[df_sentencias.index.isin(listi_ids[-1][1])].iterrows()]))\n",
        "    with open(f'split_{name}_{n_splits}_ids.pickle','wb') as file:\n",
        "        pickle.dump(listi_ids,file)\n",
        "\n",
        "    with open(f'split_{name}_{n_splits}_rows.pickle','wb') as file:\n",
        "        pickle.dump(listi_rows,file)\n",
        "    return listi_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dacc77d",
      "metadata": {
        "id": "8dacc77d"
      },
      "outputs": [],
      "source": [
        "ss = GroupKFold(n_splits=5)\n",
        "name = 'GroupKFold_alldocs'\n",
        "get_group_splits(df_sentencias, name, n_splits=5, ss=ss)\n",
        "\n",
        "name = 'StratifiedGroupKFold_alldocs'\n",
        "ss = StratifiedKFold(n_splits=5,random_state=None)\n",
        "listi = get_group_splits(df_sentencias, name, n_splits=5, ss=ss)\n",
        "\n",
        "ss = KFold(n_splits=5)\n",
        "name = 'GroupKFold_with'\n",
        "get_group_splits(df_sentencias_with, name, n_splits=5, ss=ss)\n",
        "\n",
        "name = 'StratifiedGroupKFold_with'\n",
        "ss = StratifiedGroupKFold(n_splits=5,random_state=None)\n",
        "get_group_splits(df_sentencias_with, name, n_splits=5, ss=ss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfc8b86",
      "metadata": {
        "id": "2bfc8b86"
      },
      "outputs": [],
      "source": [
        "# probar parseando sentences_with_annotations_20220323a.txt y all_sentencias_ss.html\n",
        "# en ese sabemos que están todos los párrafos que necesitamos\n",
        "import re\n",
        "ww = re.compile('<<.*>>')\n",
        "\n",
        "# los nombres de los archivos deberían matchear todos acá!\n",
        "listi = []\n",
        "with open('all_sentencias_ss.html') as fp:\n",
        "    soup = BeautifulSoup(fp, 'html.parser')\n",
        "\n",
        "for x in tqdm(soup.find('body').find_all(class_='tabla_termino')):\n",
        "    sesgo = {}\n",
        "    sesgo['concepto'] = x.find(class_='tabla_termino_columna_concepto').text\n",
        "    sesgo['context'] = x.find(class_='tabla_termino_columna_contexto').text.replace('<<','').replace('>>','')\n",
        "    sesgo['words'] = ww.search(x.find(class_='tabla_termino_columna_contexto').text).group().replace('<<','').replace('>>','')\n",
        "#     print('..',ww.search(x.find(class_='tabla_termino_columna_contexto').text).group())\n",
        "#     print(sesgo)\n",
        "    listi.append(sesgo)\n",
        "\n",
        "df_themis_all = pd.DataFrame(listi)\n",
        "df_themis_all.to_pickle('df_themis_all.pickle')\n",
        "df_themis_all.to_csv('df_themis_all.csv',index=False)\n",
        "df_themis_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd69e67c",
      "metadata": {
        "id": "fd69e67c"
      },
      "outputs": [],
      "source": [
        "listi = []\n",
        "with open('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.txt','r',encoding='utf-8') as file:\n",
        "    title = None\n",
        "    for x in file.readlines():\n",
        "        if x.startswith('####'):\n",
        "            title = x[5:].replace('\\n','')\n",
        "#             print(title)\n",
        "        else: # es un paragraph\n",
        "            if len(x) > 1:\n",
        "                listi.append({'doc':title,'text':x.replace('\\n','')})\n",
        "df_all_sentencias = pd.DataFrame(listi)\n",
        "df_all_sentencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "468b0c9e",
      "metadata": {
        "id": "468b0c9e"
      },
      "outputs": [],
      "source": [
        "# ahora hay que hacer el maching entre el text de este y el context del otro df\n",
        "ll = []\n",
        "for i in tqdm(range(0,len(df_all_sentencias))):\n",
        "    text = df_all_sentencias['text'].values[i]\n",
        "    ll.append(1 if any(x for x in df_themis_all['context'] if x in text) else 0)\n",
        "df_all_sentencias['themis'] = ll\n",
        "df_all_sentencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba0b015",
      "metadata": {
        "id": "3ba0b015"
      },
      "outputs": [],
      "source": [
        "listi = []\n",
        "for i in tqdm(range(0,len(df_sentencias))):\n",
        "    doc = df_sentencias['doc'].values[i]\n",
        "    text = df_sentencias['text'].values[i]\n",
        "    dd = df_all_sentencias[(df_all_sentencias['doc'] == doc) & (df_all_sentencias['text'] == text)]\n",
        "    listi.append(dd['themis'].values[0])\n",
        "\n",
        "df_sentencias['themis'] = listi\n",
        "df_sentencias.to_pickle('df_themis.pickle')\n",
        "df_sentencias.to_csv('df_themis.csv',index=False)\n",
        "df_sentencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "006554a2",
      "metadata": {
        "id": "006554a2"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import torch\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('PlanTL-GOB-ES/RoBERTalex')\n",
        "model = RobertaModel.from_pretrained('PlanTL-GOB-ES/RoBERTalex')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9972c44",
      "metadata": {
        "id": "c9972c44"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor, IntTensor\n",
        "import transformers\n",
        "\n",
        "listi = []\n",
        "listi_embeddings = []\n",
        "for i in tqdm(range(0,len(df_sentencias))): # 314\n",
        "    x = df_sentencias['text'].values[i]\n",
        "    metrics = {}\n",
        "    metrics['doc'] = df_sentencias['doc'].values[i]\n",
        "    metrics['page'] = df_sentencias['page'].values[i]\n",
        "    metrics['text'] = x\n",
        "    metrics['bias'] = df_sentencias['bias'].values[i]\n",
        "\n",
        "    tokens = tokenizer(x, return_tensors='pt')\n",
        "    if len(tokens[\"input_ids\"][-1]) > 512:\n",
        "        # https://arxiv.org/pdf/1905.05583.pdf\n",
        "        tokens_ = {}\n",
        "        tokens_['input_ids'] = Tensor([tokens['input_ids'][-1][:128].tolist() + tokens['input_ids'][-1][-384:].tolist()]).int()\n",
        "        tokens_['attention_mask'] = Tensor([tokens['attention_mask'][-1][:128].tolist() + tokens['attention_mask'][-1][-384:].tolist()]).to(torch.int64)\n",
        "        tokens = transformers.BatchEncoding(tokens_)\n",
        "\n",
        "    embeddings = model(**tokens)\n",
        "    embeddings = embeddings.last_hidden_state[-1][-1]\n",
        "    listi.append(metrics)\n",
        "    listi_embeddings.append(embeddings.tolist())\n",
        "    del tokens\n",
        "    del embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8952c0",
      "metadata": {
        "id": "8f8952c0"
      },
      "outputs": [],
      "source": [
        "df_st = pd.concat([pd.DataFrame(listi),pd.DataFrame(listi_embeddings)],axis=1)\n",
        "df_st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea787625",
      "metadata": {
        "id": "ea787625"
      },
      "outputs": [],
      "source": [
        "df_st.to_pickle('df_robertalex.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404970ea",
      "metadata": {
        "id": "404970ea"
      },
      "outputs": [],
      "source": [
        "df_sentencias['text'].values[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c9926f",
      "metadata": {
        "id": "03c9926f"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a283a5",
      "metadata": {
        "id": "e7a283a5"
      },
      "outputs": [],
      "source": [
        "# OJO! No es para usar antes de la división de training/test, es código de ejemplo\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "\n",
        "stop_words=set(nltk.corpus.stopwords.words('spanish'))\n",
        "def preproc(text):\n",
        "    ll = []\n",
        "    for t in nlp(text):\n",
        "        if t.is_stop or t.is_punct or not t.is_alpha or t.text in stop_words:\n",
        "            continue\n",
        "        ll.append(t.lemma_.lower())\n",
        "    return ' '.join(ll)\n",
        "\n",
        "vectorizer = TfidfVectorizer(lowercase = True, # Si querés podés agregar max features, y un par de cosas más\n",
        "                             preprocessor=preproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdef8c14",
      "metadata": {
        "id": "bdef8c14"
      },
      "outputs": [],
      "source": [
        "X = vectorizer.fit_transform(df_sentencias['text'])\n",
        "df_tf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=df_sentencias.index)\n",
        "df_tf.to_pickle('df_tfidf_full.pickle')\n",
        "df_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0191ef19",
      "metadata": {
        "id": "0191ef19"
      },
      "outputs": [],
      "source": [
        "# X = vectorizer.fit_transform(df_sentencias_with['text'])\n",
        "df_tf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=df_sentencias_with.index)\n",
        "df_tf.to_pickle('df_tfidf_with.pickle')\n",
        "df_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea47cc0",
      "metadata": {
        "id": "fea47cc0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "dir_path = './'\n",
        "\n",
        "for ff in os.listdir(dir_path):\n",
        "\n",
        "    if not ff.endswith('.pickle'):\n",
        "        continue\n",
        "\n",
        "    if not ff.startswith('df'):\n",
        "        continue\n",
        "\n",
        "    df = pd.read_pickle(dir_path + ff)\n",
        "\n",
        "    if 'doc' not in df.columns:\n",
        "        continue\n",
        "\n",
        "    print(ff)\n",
        "\n",
        "    df = df[[x for x in df.columns if x != 'doc' and x != 'page' and x != 'text' and x != 'highlight']]\n",
        "    df = df.rename(columns={'label':'bias'})\n",
        "\n",
        "    df = df[df.index.isin(df_sentencias_with.index)]\n",
        "\n",
        "    df.to_pickle(dir_path + '__anon__' + ff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1f8c03",
      "metadata": {
        "id": "4f1f8c03"
      },
      "outputs": [],
      "source": [
        "df_expli = pd.read_pickle(dir_path + '__anon__df_explicaciones.pickle')\n",
        "df_expli = df_expli[['bias','responses_ZS_expli_gemma2b','responses_ZS_expli_gpt-3.5-turbo','responses_ZS_expli_llama27b-chat','responses_ZS_expli_mistral7b-instruct','responses_bert_FS_expli_gpt-3.5-turbo_dynamic_all_False_4','responses_bert_FS_expli_mistral7b-instruct_dynamic_all_False_4','responses_bert_FS_expli_gemma2b_dynamic_all_False_4',]]\n",
        "\n",
        "df_expli = df_expli.rename(columns={'responses_ZS_expli_gemma2b':'zero-shot__gemma2b',\n",
        "'responses_ZS_expli_gpt-3.5-turbo':'zero-shot__gpt-3.5-turbo',\n",
        "'responses_ZS_expli_llama27b-chat':'zero-shot__llama27b-chat',\n",
        "'responses_ZS_expli_mistral7b-instruct':'zero-shot__mistral7b-instruct',\n",
        "'responses_bert_FS_expli_gpt-3.5-turbo_dynamic_all_False_4':'few-shot_dynamic__gpt-3.5-turbo',\n",
        "'responses_bert_FS_expli_mistral7b-instruct_dynamic_all_False_4':'few-shot_dynamic__mistral7b-instruct',\n",
        "'responses_bert_FS_expli_gemma2b_dynamic_all_False_4':'few-shot_dynamic__gemma2b'})\n",
        "df_expli.to_pickle(dir_path + '__anon__df_explicaciones.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5808bb6",
      "metadata": {
        "id": "f5808bb6"
      },
      "outputs": [],
      "source": [
        "df_expli = df_expli[['bias','few-shot_dynamic__gpt-3.5-turbo']]\n",
        "\n",
        "df_expli['lala'] = [x[0] for x in df_expli['few-shot_dynamic__gpt-3.5-turbo']]\n",
        "df_expli['expli'] = [x[1] for x in df_expli['few-shot_dynamic__gpt-3.5-turbo']]\n",
        "df_expli = df_expli[df_expli['bias'] == df_expli['lala']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e5977d",
      "metadata": {
        "id": "f5e5977d"
      },
      "outputs": [],
      "source": [
        "list(df_expli[df_expli['bias'] == 1]['expli'].values)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}