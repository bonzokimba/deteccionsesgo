{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13bd18b0",
      "metadata": {
        "id": "13bd18b0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a86167",
      "metadata": {
        "id": "f7a86167"
      },
      "outputs": [],
      "source": [
        "df_sentencias = pd.read_json('xxxxxxxxxxxxxxxxxxxxxx.jsonl',lines=True)\n",
        "df_sentencias['bias'] = [1 if len(x) > 0 else 0 for x in df_sentencias['highlight']]\n",
        "df_sentencias = df_sentencias[~df_sentencias['doc'].str.startswith('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')]\n",
        "aa = df_sentencias.groupby('doc')[['bias']].sum()\n",
        "df_sentencias = df_sentencias[df_sentencias['doc'].isin(aa[aa['bias'] > 0].index)]\n",
        "df_sentencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00103dab",
      "metadata": {
        "id": "00103dab"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_metrics(pred,y_test):\n",
        "    metrics = {}\n",
        "\n",
        "#     print(pred)\n",
        "#     print(y_test)\n",
        "#     print('-----------------------')\n",
        "\n",
        "    metrics['accuracy'] = accuracy_score(y_test,pred)\n",
        "\n",
        "    metrics['precision'] = precision_score(y_test,pred,average='binary')\n",
        "    metrics['recall'] = recall_score(y_test,pred,average='binary')\n",
        "    metrics['fmeasure'] = f1_score(y_test,pred,average='binary')\n",
        "\n",
        "    metrics['precision_weighted'] = precision_score(y_test,pred,average='weighted')\n",
        "    metrics['recall_weighted'] = recall_score(y_test,pred,average='weighted')\n",
        "    metrics['fmeasure_weighted'] = f1_score(y_test,pred,average='weighted')\n",
        "    metrics['average_precision_score_weighted'] = average_precision_score(y_test,pred,average='weighted')\n",
        "    if len(set(y_test)) != 1:\n",
        "        metrics['roc_auc_score_weighted'] = roc_auc_score(y_test,pred,average='weighted')\n",
        "\n",
        "    metrics['precision_micro'] = precision_score(y_test,pred,average='micro')\n",
        "    metrics['recall_micro'] = recall_score(y_test,pred,average='micro')\n",
        "    metrics['fmeasure_micro'] = f1_score(y_test,pred,average='micro')\n",
        "    metrics['average_precision_score_micro'] = average_precision_score(y_test,pred,average='micro')\n",
        "    if len(set(y_test)) != 1:\n",
        "        metrics['roc_auc_score_micro'] = roc_auc_score(y_test,pred,average='micro')\n",
        "\n",
        "    metrics['precision_macro'] = precision_score(y_test,pred,average='macro')\n",
        "    metrics['recall_macro'] = recall_score(y_test,pred,average='macro')\n",
        "    metrics['fmeasure_macro'] = f1_score(y_test,pred,average='macro')\n",
        "    metrics['average_precision_score_macro'] = average_precision_score(y_test,pred,average='macro')\n",
        "    if len(set(y_test)) != 1:\n",
        "        metrics['roc_auc_score_macro'] = roc_auc_score(y_test,pred,average='macro')\n",
        "\n",
        "    metrics['matthews_corrcoef'] = matthews_corrcoef(y_test,pred) # 0 es random prediction, 1 es perfecto, -1 inverse prediction\n",
        "\n",
        "    metrics['y_reales'] = sum(y_test)\n",
        "    metrics['y_pred'] = sum(pred)\n",
        "    metrics['total'] = len(pred)\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28525541",
      "metadata": {
        "scrolled": true,
        "id": "28525541"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from json import JSONDecodeError\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "path_dir = 'results/'\n",
        "\n",
        "import re\n",
        "re_json = re.compile('\\{.*\\}')\n",
        "\n",
        "listi_metrics = []\n",
        "\n",
        "from collections import defaultdict\n",
        "categories = defaultdict(dict)\n",
        "\n",
        "for ff in os.listdir(path_dir):\n",
        "\n",
        "    if not ff.startswith('responses'):\n",
        "        continue\n",
        "\n",
        "    with open(path_dir + ff,'rb') as file:\n",
        "        dd = pickle.load(file)\n",
        "\n",
        "    print('-----------------------------',ff)\n",
        "\n",
        "    listi = []\n",
        "    for x in dd:\n",
        "        cc = str(x[1] if isinstance(x[1],str) else x[1].content).replace('\\n',' ')\n",
        "#         print(cc)\n",
        "        cc = re_json.search(cc)\n",
        "#         print(cc)\n",
        "        if cc is None:\n",
        "            continue\n",
        "        cc = cc.group().replace('{\\'','{\"').replace('\\'}','\"}').replace('\\':','\":').replace(':\\'',':\"').replace(': \\'',':\"').replace('`','\"').replace('\"\"','\"').replace('\"\"','\"')\n",
        "#         .replace(' \\'','\"').replace('\\' ','\"')\n",
        "        cc = re.sub('\\s+',' ',cc).replace('\" \"','\", \"').replace('\". \"','\", \"').replace('\"; \"','\", \"')\n",
        "        try:\n",
        "            cc = json.loads(cc)\n",
        "        except JSONDecodeError as e:\n",
        "            print(cc)\n",
        "            print('_________________________',e)\n",
        "            cc = None\n",
        "\n",
        "        if cc is None:\n",
        "            continue\n",
        "        ll = None\n",
        "        if 'label' in cc: ll = 'label'\n",
        "        elif 'categoria' in cc: ll = 'categoria'\n",
        "        elif 'categoría' in cc: ll = 'categoría'\n",
        "        elif 'categorias' in cc: ll = 'categorias'\n",
        "        elif 'categorías' in cc: ll = 'categorías'\n",
        "        elif 'category' in cc: ll = 'category'\n",
        "        elif 'sesgo' in cc: ll = 'sesgo'\n",
        "        else:\n",
        "            print(df_sentencias[df_sentencias.index == x[0]]['bias'].values[0],'--',cc.keys())\n",
        "            continue\n",
        "\n",
        "        if ll is None:\n",
        "            continue\n",
        "\n",
        "        if isinstance(cc[ll],bool):\n",
        "            cc[ll] = 'sesgo' if cc[ll] else 'no-sesgo'\n",
        "\n",
        "        if not isinstance(cc[ll],str):\n",
        "            continue\n",
        "\n",
        "#         print(cc[ll])\n",
        "        cc = 1 if (cc[ll].lower() == 'sesgo' or cc[ll].lower() == 'bias') else 0 if (cc[ll].lower() == 'no-sesgo' or cc[ll].lower() == 'no-bias') else -1\n",
        "        if cc == -1:\n",
        "            continue\n",
        "        listi.append({'index':x[0],'y_pred':cc})\n",
        "#     print(listi)\n",
        "    df_pred = pd.DataFrame(listi).set_index('index')\n",
        "\n",
        "    df_merge = df_sentencias.merge(df_pred,left_index=True,right_index=True)\n",
        "    metrics = compute_metrics(df_merge['y_pred'],df_merge['bias'])\n",
        "    if 'fold' in ff:\n",
        "        metrics['which'] = '_'.join(ff.split('_')[0:-1])\n",
        "        metrics['fold'] = int(ff[-1])\n",
        "    else:\n",
        "        metrics['which'] = ff\n",
        "\n",
        "    categories[metrics['which']].update(df_merge[['y_pred']].to_dict()['y_pred'])\n",
        "    listi_metrics.append(metrics)\n",
        "#     print()\n",
        "#     print(metrics)\n",
        "#     print()\n",
        "\n",
        "dd = pd.DataFrame(listi_metrics)\n",
        "dd.to_csv('__results_folds.csv')\n",
        "ee = dd.groupby('which').mean().sort_values('precision',ascending=False)\n",
        "ee.to_csv('__results_folds_avg.csv')\n",
        "ee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7171753d",
      "metadata": {
        "id": "7171753d"
      },
      "outputs": [],
      "source": [
        "# dd.sort_values('precision',ascending=False)\n",
        "df_categories = pd.DataFrame.from_dict(categories)\n",
        "df_categories = df_categories.join(df_sentencias[['bias']])\n",
        "\n",
        "ll = []\n",
        "for i in tqdm(range(0,len(df_categories.columns))):\n",
        "    x = df_categories.columns[i]\n",
        "    for j in range(i+1,len(df_categories.columns)):\n",
        "        y = df_categories.columns[j]\n",
        "        if x == 'bias' or y == 'bias':\n",
        "            continue\n",
        "        aa = [1 if z[0] == 1 and z[1] == 1 else 0 for z in zip(df_categories[x],df_categories[y])]\n",
        "        mm = compute_metrics(aa,df_categories['bias'])\n",
        "        mm['which_A'] = x\n",
        "        mm['which_B'] = y\n",
        "        ll.append(mm)\n",
        "\n",
        "pd.DataFrame(ll).sort_values('recall',ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7ea69b",
      "metadata": {
        "id": "8b7ea69b"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from scipy.stats import mannwhitneyu\n",
        "from scipy.stats import wilcoxon\n",
        "\n",
        "from statistics import mean, stdev\n",
        "from math import sqrt\n",
        "\n",
        "def cohens_d(c0,c1):\n",
        "    den = sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2)\n",
        "    if den == 0:\n",
        "        return '-'\n",
        "    cohens_d = abs((mean(c0) - mean(c1)) / den)\n",
        "    if cohens_d < 0.1:\n",
        "        return 'negligible'\n",
        "    if cohens_d < 0.35:\n",
        "        return 'small'\n",
        "    if cohens_d < 0.65:\n",
        "        return 'medium'\n",
        "    return 'large'\n",
        "\n",
        "def paired_analysis(f1,f2,alpha=0.01): # only one df is needed, then is for i for j\n",
        "\n",
        "    if len(f1) != len(f2) or len(f1) == 1:\n",
        "        return None\n",
        "\n",
        "    normal = False\n",
        "    try:\n",
        "         if scipy.stats.normaltest(f1,nan_policy='omit').pvalue < alpha and scipy.stats.normaltest(f2,nan_policy='omit').pvalue < alpha:\n",
        "                normal = True\n",
        "    except:\n",
        "        pass\n",
        "    if not normal:\n",
        "        statsb = wilcoxon(f1,f2,alternative='two-sided',zero_method='zsplit')\n",
        "        statsg = wilcoxon(f1,f2,alternative='greater',zero_method='zsplit')\n",
        "        statsl = wilcoxon(f1,f2,alternative='less',zero_method='zsplit')\n",
        "        return (statsb.pvalue,statsg.pvalue,statsl.pvalue,cohens_d(f1,f2))\n",
        "    else: # both normal\n",
        "        statsb = scipy.stats.ttest_rel(f1,f2,nan_policy='omit')\n",
        "#         print(statsb)\n",
        "        return (statsb.statistic,statsb.pvalue,cohens_d(f1,f2))\n",
        "\n",
        "    return differents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df60c263",
      "metadata": {
        "id": "df60c263"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ee = dd[dd['fold'] == dd['fold']]\n",
        "ee = ee.drop(columns=['y_reales','y_pred','total','fold'])\n",
        "ee_list = list(ee['which'].unique())\n",
        "\n",
        "listi_metrics = []\n",
        "alpha = 0.01\n",
        "\n",
        "for x in tqdm(ee_list):\n",
        "\n",
        "    ee_x = ee[ee['which'] == x].drop(columns=['which'])\n",
        "\n",
        "    for y in ee_list:\n",
        "        if x == y:\n",
        "            continue\n",
        "\n",
        "        ee_y =  ee[ee['which'] == y].drop(columns=['which'])\n",
        "\n",
        "\n",
        "        dicti = {}\n",
        "        dicti['X'] = x\n",
        "        dicti['Y'] = y\n",
        "\n",
        "#         print(x,y)\n",
        "\n",
        "        for c in ee_x.columns:\n",
        "\n",
        "            stats = paired_analysis(ee_x[c],ee_y[c],alpha)\n",
        "            if stats is None:\n",
        "                continue\n",
        "            if len(stats) < 4: # normal -- tiene que dividir y chequear para ver cual queremos rejectear\n",
        "                dicti[c] = stats[2] if stats[1]/2 < alpha and stats[0] > 0 else '-'\n",
        "            else:\n",
        "                dicti[c] = stats[3] if stats[1] < alpha else '-'\n",
        "\n",
        "        if len(dicti) != 2:\n",
        "            listi_metrics.append(dicti)\n",
        "\n",
        "df_stats = pd.DataFrame(listi_metrics)\n",
        "df_stats.to_csv('df_stats_fold.csv',index=False)\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caa252e",
      "metadata": {
        "id": "1caa252e"
      },
      "outputs": [],
      "source": [
        "# definir ff para juntar todas las combinaciones !\n",
        "ff = pd.read_csv('all_best_metrics_df.csv',sep=';')\n",
        "ff = pd.concat([ff,pd.read_csv('__results_folds_avg.csv')])\n",
        "ff = ff.dropna(axis=1).set_index('which').T\n",
        "ff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e04088",
      "metadata": {
        "id": "b6e04088"
      },
      "outputs": [],
      "source": [
        "# ff = dd.groupby('which').mean().sort_values('precision',ascending=False).drop(columns=['y_reales','y_pred','total','fold']).T\n",
        "\n",
        "alpha = 0.01\n",
        "listi = []\n",
        "for x in tqdm(ff.columns):\n",
        "    for y in ff.columns:\n",
        "        if x == y:\n",
        "            continue\n",
        "        stats = paired_analysis(ff[x],ff[y],alpha)\n",
        "        if stats is None:\n",
        "            continue\n",
        "\n",
        "        dicti = {}\n",
        "        dicti['X'] = x\n",
        "        dicti['Y'] = y\n",
        "        if len(stats) < 4: # normal -- tiene que dividir y chequear para ver cual queremos rejectear\n",
        "            z = stats[2] if stats[1]/2 < alpha and stats[0] > 0 else '-'\n",
        "        else:\n",
        "            z = stats[3] if stats[1] < alpha else '-'\n",
        "\n",
        "        dicti['Z'] = z\n",
        "        listi.append(dicti)\n",
        "\n",
        "df_stats_full = pd.DataFrame(listi)\n",
        "df_stats_full.to_csv('df_stats_full.csv',index=False)\n",
        "df_stats_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab470c8",
      "metadata": {
        "id": "6ab470c8"
      },
      "outputs": [],
      "source": [
        "import ranky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10499ff1",
      "metadata": {
        "id": "10499ff1"
      },
      "outputs": [],
      "source": [
        "def get_best_params_(df_combs,metric=None,top=15):\n",
        "    if metric is not None:\n",
        "        if 'user_attrs_metric' in df_combs.columns:\n",
        "            model = df_combs[df_combs['user_attrs_metric'] == metric]\n",
        "        else:\n",
        "            model = df_combs[df_combs['target_metric'] == metric]\n",
        "    else:\n",
        "        model = df_combs\n",
        "\n",
        "    if 'user_attrs_1-precision' in df_combs.columns:\n",
        "        metrics_to_rank = ['user_attrs_1-precision','user_attrs_1-recall','user_attrs_metric']\n",
        "    else:\n",
        "        metrics_to_rank = ['1.precision','1.recall','mcc']\n",
        "    model_metrics = model[metrics_to_rank]\n",
        "    rank = ranky.pairwise(model_metrics).to_dict()\n",
        "    rank = [(k, v) for k, v in rank.items()]\n",
        "    rank.sort(key=lambda x: -x[1])\n",
        "    best_params_top = []\n",
        "    for i in range(0,min(top,len(rank))):\n",
        "        best_params_top.append(model[model.index == rank[i][0]].to_dict('records')[0])\n",
        "    return pd.DataFrame(best_params_top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2adcabc1",
      "metadata": {
        "id": "2adcabc1"
      },
      "outputs": [],
      "source": [
        "def get_best_params(path,top=15,metrics=None):\n",
        "\n",
        "    if metrics is None:\n",
        "        metrics = ['PRAUC','Precision','Recall','MCC']\n",
        "\n",
        "    df_combs = pd.read_csv(path)\n",
        "    if 'user_attrs_1-precision' in df_combs.columns:\n",
        "        df_combs['user_attrs_1-fmeasure'] = [(2*df_combs['user_attrs_1-precision'].values[i] * df_combs['user_attrs_1-recall'].values[i])/(df_combs['user_attrs_1-precision'].values[i] + df_combs['user_attrs_1-recall'].values[i]) for i in range(0,len(df_combs))]\n",
        "        df_combs.sort_values('user_attrs_1-fmeasure',ascending=False)\n",
        "\n",
        "    ll = {}\n",
        "    for m in metrics:\n",
        "        print(path, '::',m)\n",
        "        ll[m] = get_best_params_(df_combs,metric=m,top=top)\n",
        "\n",
        "    return ll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df9a97be",
      "metadata": {
        "id": "df9a97be"
      },
      "outputs": [],
      "source": [
        "best_pp = {}\n",
        "# best_pp['bertlegal4'] = get_best_params('bertlegal4-studies_df.csv')\n",
        "# best_pp['multilingua4'] = get_best_params('multilingua4-studies_df.csv')\n",
        "# best_pp['beto4'] = get_best_params('beto4-studies_df.csv')\n",
        "# best_pp['tfidf'] = get_best_params('tfidf-studies_df.csv')\n",
        "\n",
        "best_pp['multilingua4'] = get_best_params('multilingua-all_best_metrics_df.csv')\n",
        "best_pp['bertlegal4'] = get_best_params('bertlegal-all_best_metrics_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be936329",
      "metadata": {
        "id": "be936329"
      },
      "outputs": [],
      "source": [
        "alldd = pd.DataFrame()\n",
        "for k in best_pp:\n",
        "    bp = best_pp[k]\n",
        "    dd = pd.DataFrame()\n",
        "    for m in bp:\n",
        "        bp[m]['which'] = k\n",
        "        dd = pd.concat([dd,bp[m]])\n",
        "    if 'user_attrs_1-fmeasure' in dd.columns:\n",
        "        dd = dd.sort_values(by=['user_attrs_1-fmeasure'],ascending=False)#[20:35]\n",
        "    else:\n",
        "        dd = dd.sort_values(by=['1.f1-score'],ascending=False)#[20:35]\n",
        "    alldd = pd.concat([alldd,dd])\n",
        "alldd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3eb308",
      "metadata": {
        "id": "bd3eb308"
      },
      "outputs": [],
      "source": [
        "# alldd.sort_values(['user_attrs_1-precision','user_attrs_1-recall'],ascending=False)\n",
        "# alldd.sort_values(['user_attrs_1-fmeasure'],ascending=False).to_csv('best_params_tfidf.csv',index=False)\n",
        "alldd.to_csv('best_params_legal_multi.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7020b1e6",
      "metadata": {
        "id": "7020b1e6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab62c62",
      "metadata": {
        "id": "7ab62c62"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from json import JSONDecodeError\n",
        "import pickle\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "path_dir = '/results/'\n",
        "\n",
        "import re\n",
        "re_json = re.compile('\\{.*\\}')\n",
        "\n",
        "listi = []\n",
        "from collections import defaultdict\n",
        "categories = defaultdict(dict)\n",
        "\n",
        "for ff in tqdm(os.listdir(path_dir)):\n",
        "\n",
        "    if not ff.startswith('responses'):\n",
        "        continue\n",
        "\n",
        "    if 'expli' not in ff: # solo los que pedimos que tuvieran\n",
        "        continue\n",
        "\n",
        "    print(ff)\n",
        "\n",
        "    with open(path_dir + ff,'rb') as file:\n",
        "        dd = pickle.load(file)\n",
        "\n",
        "    for x in dd:\n",
        "        cc = str(x[1] if isinstance(x[1],str) else x[1].content).replace('\\n',' ')\n",
        "#         print(cc)\n",
        "        cc = re_json.search(cc)\n",
        "#         print(cc)\n",
        "        if cc is None:\n",
        "            continue\n",
        "        cc = cc.group().replace('{\\'','{\"').replace('\\'}','\"}').replace('\\':','\":').replace(':\\'',':\"').replace(': \\'',':\"').replace('`','\"').replace('\"\"','\"').replace('\"\"','\"')\n",
        "#         .replace(' \\'','\"').replace('\\' ','\"')\n",
        "        cc = re.sub('\\s+',' ',cc).replace('\" \"','\", \"').replace('\". \"','\", \"').replace('\"; \"','\", \"')\n",
        "        try:\n",
        "            cc = json.loads(cc)\n",
        "        except JSONDecodeError as e:\n",
        "#             print(cc)\n",
        "#             print('_________________________',e)\n",
        "            cc = None\n",
        "\n",
        "        if cc is None:\n",
        "            continue\n",
        "\n",
        "        l = None # este es para todos\n",
        "        if 'explicación' in cc: l = 'explicación'\n",
        "        elif 'explanation' in cc: l = 'explanation'\n",
        "        elif 'explicacion' in cc: l = 'explicacion'\n",
        "        else:\n",
        "            print(cc.keys())\n",
        "            continue\n",
        "\n",
        "        if l is None:\n",
        "            continue\n",
        "\n",
        "        ll = None\n",
        "        if 'label' in cc: ll = 'label'\n",
        "        elif 'categoria' in cc: ll = 'categoria'\n",
        "        elif 'categoría' in cc: ll = 'categoría'\n",
        "        elif 'categorias' in cc: ll = 'categorias'\n",
        "        elif 'categorías' in cc: ll = 'categorías'\n",
        "        elif 'category' in cc: ll = 'category'\n",
        "        elif 'sesgo' in cc: ll = 'sesgo'\n",
        "\n",
        "        if ll is None:\n",
        "            continue\n",
        "        if isinstance(cc[ll],bool):\n",
        "            print(cc)\n",
        "        ll = 1 if (isinstance(cc[ll],bool) and cc[ll]) else 0 if (isinstance(cc[ll],bool) and not cc[ll]) else 1 if (cc[ll].lower() == 'sesgo' or cc[ll].lower() == 'bias') else 0 if (cc[ll].lower() == 'no-sesgo' or cc[ll].lower() == 'no-bias') else -1\n",
        "        if ll == -1:\n",
        "            continue\n",
        "\n",
        "        listi.append({'index':x[0],'text': df_sentencias[df_sentencias.index == x[0]]['text'].values[0], 'doc': df_sentencias[df_sentencias.index == x[0]]['doc'].values[0], 'label': df_sentencias[df_sentencias.index == x[0]]['bias'].values[0], 'explicación':(ll,cc[l]),'which':ff})\n",
        "\n",
        "df_expli = pd.DataFrame(listi).pivot(columns='which',values='explicación',index=['index','text','label','doc'])\n",
        "df_expli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0284856",
      "metadata": {
        "id": "b0284856"
      },
      "outputs": [],
      "source": [
        "df_expli.reset_index().set_index('index').to_pickle('df_explicaciones.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3781a43e",
      "metadata": {
        "id": "3781a43e"
      },
      "outputs": [],
      "source": [
        "df_expli.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b669f4",
      "metadata": {
        "id": "02b669f4"
      },
      "outputs": [],
      "source": [
        "aa = pd.read_csv('tfidf-studies_df.csv')\n",
        "aa['user_attrs_1-fmeasure'] = [(2*aa['user_attrs_1-precision'].values[i] * aa['user_attrs_1-recall'].values[i])/(aa['user_attrs_1-precision'].values[i] + aa['user_attrs_1-recall'].values[i]) for i in range(0,len(aa))]\n",
        "aa = aa.sort_values('user_attrs_1-fmeasure',ascending=False)\n",
        "aa[aa['user_attrs_1-fmeasure'] < 0.45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14955038",
      "metadata": {
        "id": "14955038"
      },
      "outputs": [],
      "source": [
        "# hacer el de popularidad\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "du = DummyClassifier(strategy='most_frequent')\n",
        "\n",
        "bias = [1 if x == 0 else 0 for x in df_sentencias['bias']]\n",
        "\n",
        "du.fit(df_sentencias['text'],bias)\n",
        "compute_metrics(du.predict(df_sentencias['text']).tolist(),bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e565909",
      "metadata": {
        "id": "5e565909"
      },
      "outputs": [],
      "source": [
        "# hacer la tabla para latex\n",
        "\n",
        "df = pd.read_excel('results.xlsx',sheet_name='Traditional')\n",
        "df = df[['which','precision','recall','precision_weighted','recall_weighted','matthews_corrcoef']]\n",
        "df = df.dropna(how='all').dropna(axis=0)\n",
        "df\n",
        "\n",
        "out = []\n",
        "out.append(' & '.join(df.columns))\n",
        "for row in df.iterrows():\n",
        "    ll = row[1].to_dict()\n",
        "    ll = [v if isinstance(v,str) else str(round(v,3)) for k,v in ll.items()]\n",
        "    out.append(' & '.join(ll) + ' \\\\tabularnewline')\n",
        "print('\\n'.join(out))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}