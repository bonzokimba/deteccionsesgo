{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I73kZS83ABww"
      },
      "outputs": [],
      "source": [
        "# Importo las librerias\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "# si no instalo esto no puedo seguir despues\n",
        "#!pip install spacy &> /dev/null\n",
        "#!python -m spacy download es_core_news_lg &> /dev/null\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNK0K0SYABw0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.pkl')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icnnr2iVABw1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import FastText\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "print(\"FastText + CatBoost + sin smote\")\n",
        "\n",
        "# Verificación de la distribución de etiquetas\n",
        "print(\"Distribución de etiquetas:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Convertir el texto a listas de palabras para entrenar FastText\n",
        "corpus = [doc.split() for doc in X]\n",
        "\n",
        "# Entrenar el modelo FastText\n",
        "ft_model = FastText(sentences=corpus, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Crear un vector promedio para cada documento usando las palabras que aparecen en el modelo FastText\n",
        "def get_fasttext_embeddings(text, model, vector_size):\n",
        "    # Vector de ceros para las palabras que no están en el vocabulario\n",
        "    embeddings = np.zeros((len(text), vector_size))\n",
        "\n",
        "    for i, doc in enumerate(text):\n",
        "        words = doc.split()\n",
        "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "\n",
        "        if len(word_vectors) > 0:\n",
        "            embeddings[i] = np.mean(word_vectors, axis=0)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Crear las representaciones vectoriales de los textos\n",
        "X_ft = get_fasttext_embeddings(X, ft_model, vector_size=300)\n",
        "\n",
        "# Verificar los embeddings generados\n",
        "print(\"Primeros 5 embeddings generados:\")\n",
        "print(X_ft[:5])  # Revisa las primeras 5 representaciones vectoriales\n",
        "\n",
        "# Normalizar los vectores para que no haya valores negativos\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_ft_normalized = scaler.fit_transform(X_ft)\n",
        "\n",
        "# Definir el modelo de clasificación CatBoost\n",
        "catboost_model = CatBoostClassifier(cat_features=[], random_seed=42, verbose=0)\n",
        "\n",
        "# Realizar un ajuste rápido del modelo para verificar las predicciones\n",
        "catboost_model.fit(X_ft_normalized, y)\n",
        "predictions = catboost_model.predict(X_ft_normalized)\n",
        "\n",
        "# Verificar las predicciones iniciales\n",
        "print(\"Primeras 10 predicciones:\")\n",
        "print(predictions[:10])\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score, zero_division=1),\n",
        "    'recall': make_scorer(recall_score, zero_division=1),\n",
        "    'precision': make_scorer(precision_score, zero_division=1),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usar StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Realizar validación cruzada\n",
        "final_scores = cross_validate(catboost_model, X_ft_normalized, y, cv=skf, scoring=scoring)\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXu9ue-cABw4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "print(\"Word2Vec + Random Forest con SMOTE y Optimización de Hiperparámetros\")\n",
        "\n",
        "# Verificación de la distribución de etiquetas\n",
        "print(\"Distribución de etiquetas:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Convertir el texto a listas de palabras para entrenar Word2Vec\n",
        "corpus = [doc.split() for doc in X]\n",
        "\n",
        "# Entrenar el modelo Word2Vec\n",
        "w2v_model = Word2Vec(sentences=corpus, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Crear un vector promedio para cada documento usando las palabras que aparecen en el modelo Word2Vec\n",
        "def get_word2vec_embeddings(text, model, vector_size):\n",
        "    embeddings = np.zeros((len(text), vector_size))\n",
        "\n",
        "    for i, doc in enumerate(text):\n",
        "        words = doc.split()\n",
        "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "\n",
        "        if len(word_vectors) > 0:\n",
        "            embeddings[i] = np.mean(word_vectors, axis=0)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Crear las representaciones vectoriales de los textos\n",
        "X_w2v = get_word2vec_embeddings(X, w2v_model, vector_size=300)\n",
        "\n",
        "# Normalizar los vectores para que no haya valores negativos\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_w2v_normalized = scaler.fit_transform(X_w2v)\n",
        "\n",
        "# Aplicar SMOTE para balancear las clases\n",
        "smote = SMOTE(random_state=42)\n",
        "X_w2v_resampled, y_resampled = smote.fit_resample(X_w2v_normalized, y)\n",
        "\n",
        "# Definir los hiperparámetros a optimizar para Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Definir el modelo de clasificación Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score, zero_division=1),\n",
        "    'recall': make_scorer(recall_score, zero_division=1),\n",
        "    'precision': make_scorer(precision_score, zero_division=1),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usar StratifiedKFold para la validación cruzada\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Optimización de hiperparámetros con GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
        "                           scoring=scoring, refit='f1', cv=skf, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Realizar la búsqueda de los mejores hiperparámetros\n",
        "grid_search.fit(X_w2v_resampled, y_resampled)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "\n",
        "# Evaluar el modelo con los mejores hiperparámetros usando validación cruzada\n",
        "final_scores = cross_validate(grid_search.best_estimator_, X_w2v_resampled, y_resampled, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpPUc8bWABw5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import FastText\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "\n",
        "print(\"FastText + CatBoost con SMOTE\")\n",
        "\n",
        "# Verificación de la distribución de etiquetas\n",
        "print(\"Distribución de etiquetas antes de SMOTE:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Convertir el texto a listas de palabras para entrenar FastText\n",
        "corpus = [doc.split() for doc in X]\n",
        "\n",
        "# Entrenar el modelo FastText\n",
        "ft_model = FastText(sentences=corpus, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Crear un vector promedio para cada documento usando las palabras que aparecen en el modelo FastText\n",
        "def get_fasttext_embeddings(text, model, vector_size):\n",
        "    # Vector de ceros para las palabras que no están en el vocabulario\n",
        "    embeddings = np.zeros((len(text), vector_size))\n",
        "\n",
        "    for i, doc in enumerate(text):\n",
        "        words = doc.split()\n",
        "        word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "\n",
        "        if len(word_vectors) > 0:\n",
        "            embeddings[i] = np.mean(word_vectors, axis=0)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Crear las representaciones vectoriales de los textos\n",
        "X_ft = get_fasttext_embeddings(X, ft_model, vector_size=300)\n",
        "\n",
        "# Verificar los embeddings generados\n",
        "print(\"Primeros 5 embeddings generados:\")\n",
        "print(X_ft[:5])  # Revisa las primeras 5 representaciones vectoriales\n",
        "\n",
        "# Normalizar los vectores para que no haya valores negativos\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_ft_normalized = scaler.fit_transform(X_ft)\n",
        "\n",
        "# Aplicar SMOTE para balancear las clases\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_ft_normalized, y)\n",
        "\n",
        "# Verificar la nueva distribución de las etiquetas después de SMOTE\n",
        "print(\"Distribución de etiquetas después de SMOTE:\")\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "\n",
        "# Definir el modelo de clasificación CatBoost\n",
        "catboost_model = CatBoostClassifier(cat_features=[], random_seed=42, verbose=0)\n",
        "\n",
        "# Realizar un ajuste rápido del modelo para verificar las predicciones\n",
        "catboost_model.fit(X_resampled, y_resampled)\n",
        "predictions = catboost_model.predict(X_resampled)\n",
        "\n",
        "# Verificar las predicciones iniciales\n",
        "print(\"Primeras 10 predicciones:\")\n",
        "print(predictions[:10])\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score, zero_division=1),\n",
        "    'recall': make_scorer(recall_score, zero_division=1),\n",
        "    'precision': make_scorer(precision_score, zero_division=1),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usar StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# Reducimos el rango de hiperparámetros\n",
        "param_grid = {\n",
        "    'catboost__learning_rate': [0.05, 0.1],  # Reducimos el rango de tasas de aprendizaje\n",
        "    'catboost__iterations': [100, 200],  # Menos iteraciones para acelerar\n",
        "    'catboost__depth': [6]  # Usamos un solo valor de profundidad para acortar\n",
        "}\n",
        "# Realizar validación cruzada\n",
        "final_scores = cross_validate(catboost_model, X_resampled, y_resampled, cv=skf, scoring=scoring)\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "#grid_search.fit(X, y)\n",
        "#print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlNUClr2ABw7"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V43dyd4ABw9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"BOW LOGREG SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Regresión Logística\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rso6p3NABw-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar el pipeline de imblearn para manejar SMOTE\n",
        "\n",
        "print(\"BOW LOGREG CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización, SMOTE y clasificación usando Regresión Logística\n",
        "pipeline = ImbPipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicar SMOTE para balancear las clases\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdvDQusKABw_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar el pipeline de imblearn para manejar SMOTE\n",
        "\n",
        "\n",
        "print(\"BOW SVM CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización, SMOTE y clasificación usando SVM\n",
        "pipeline = ImbPipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicar SMOTE para balancear las clases\n",
        "    ('svm', SVC(random_state=42))  # Support Vector Machine (SVM)\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo SVM\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly'],  # Tipo de kernel\n",
        "    'svm__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQjCM9itABxA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"BOW SVM SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando SVM (sin SMOTE)\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('svm', SVC(random_state=42))  # Support Vector Machine (SVM)\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo SVM\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly'],  # Tipo de kernel\n",
        "    'svm__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7yMiGTXABxC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"BOW Random Forest SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Random Forest (sin SMOTE)\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Random Forest\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [50, 100, 200],  # Número de árboles\n",
        "    'rf__max_depth': [None, 10, 20, 30],  # Profundidad máxima de los árboles\n",
        "    'rf__min_samples_split': [2, 5, 10],  # Mínimo número de muestras para dividir un nodo\n",
        "    'rf__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50_Q8hW6ABxD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE  # Importar SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar el pipeline de imblearn\n",
        "\n",
        "print(\"BOW Random Forest CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Random Forest con SMOTE\n",
        "pipeline = ImbPipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para balancear las clases\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Random Forest\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [50, 100, 200],  # Número de árboles\n",
        "    'rf__max_depth': [None, 10, 20, 30],  # Profundidad máxima de los árboles\n",
        "    'rf__min_samples_split': [2, 5, 10],  # Mínimo número de muestras para dividir un nodo\n",
        "    'rf__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lunlsScABxE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE  # Importar SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar el pipeline de imblearn\n",
        "\n",
        "print(\"BOW Naive Bayes CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Naive Bayes con SMOTE\n",
        "pipeline = ImbPipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para balancear las clases\n",
        "    ('nb', MultinomialNB())  # Naive Bayes\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Naive Bayes\n",
        "param_grid = {\n",
        "    'nb__alpha': [0.01, 0.1, 1, 10],  # Parámetro de suavizado\n",
        "    'nb__fit_prior': [True, False]  # Ajuste de priors\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYasEp42ABxF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"BOW Naive Bayes SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Naive Bayes\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('nb', MultinomialNB())  # Naive Bayes\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Naive Bayes\n",
        "param_grid = {\n",
        "    'nb__alpha': [0.01, 0.1, 1, 10],  # Parámetro de suavizado\n",
        "    'nb__fit_prior': [True, False]  # Ajuste de priors\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yyV_QppABxH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"BOW CatBoost SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando CatBoost\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('catboost', CatBoostClassifier(learning_rate=0.1, iterations=200, depth=6, random_state=42, verbose=0))  # CatBoost\n",
        "])\n",
        "\n",
        "# Reducimos el rango de hiperparámetros\n",
        "param_grid = {\n",
        "    'catboost__learning_rate': [0.05, 0.1],  # Reducimos el rango de tasas de aprendizaje\n",
        "    'catboost__iterations': [100, 200],  # Menos iteraciones para acelerar\n",
        "    'catboost__depth': [6]  # Usamos un solo valor de profundidad para acortar\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Reducimos el número de splits a 3\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con menos combinaciones para acelerar\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFAHmkEUABxI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"BOW CatBoost CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización, SMOTE y clasificación usando CatBoost\n",
        "pipeline = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # Bag of Words (BoW)\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para balanceo de clases\n",
        "    ('catboost', CatBoostClassifier(learning_rate=0.1, iterations=200, depth=6, random_state=42, verbose=0))  # CatBoost\n",
        "])\n",
        "\n",
        "# Reducimos el rango de hiperparámetros\n",
        "param_grid = {\n",
        "    'catboost__learning_rate': [0.05, 0.1],  # Reducimos el rango de tasas de aprendizaje\n",
        "    'catboost__iterations': [100, 200],  # Menos iteraciones para acelerar\n",
        "    'catboost__depth': [6]  # Usamos un solo valor de profundidad para acortar\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Reducimos el número de splits a 3\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con menos combinaciones para acelerar\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quSZGqolABxJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "print(\"TF-IDF LOGREG SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Regresión Logística\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WOlYHkeABxJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE  # Para usar SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Para manejar el pipeline con SMOTE\n",
        "\n",
        "\n",
        "print(\"TF-IDF LOGREG CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización, SMOTE y clasificación usando Regresión Logística\n",
        "pipeline = ImbPipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para manejar el desbalance de clases\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases en la validación\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con SMOTE\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dyp6ohd0ABxK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.svm import SVC  # Para SVM\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE  # Para usar SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Para manejar el pipeline con SMOTE\n",
        "\n",
        "\n",
        "print(\"TF-IDF SVM CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización, SMOTE y clasificación usando SVM\n",
        "pipeline = ImbPipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para manejar el desbalance de clases\n",
        "    ('svm', SVC(random_state=42))  # Clasificación con SVM\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo SVM\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'svm__kernel': ['linear', 'rbf'],  # Tipos de kernel\n",
        "    'svm__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases en la validación\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con SMOTE\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWxE7OKDABxL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.svm import SVC  # Para SVM\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"TF-IDF SVM SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando SVM\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('svm', SVC(random_state=42))  # Clasificación con SVM\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo SVM\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'svm__kernel': ['linear', 'rbf'],  # Tipos de kernel\n",
        "    'svm__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases en la validación\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin SMOTE\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_33qnJNABxL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier  # Para Random Forest\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"TF-IDF RANDOM FOREST SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Random Forest\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Clasificación con Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Random Forest\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [100, 200, 500],  # Número de árboles en el bosque\n",
        "    'rf__max_depth': [None, 10, 20, 30],  # Profundidad máxima de los árboles\n",
        "    'rf__min_samples_split': [2, 5, 10],  # Mínimo número de muestras requeridas para dividir un nodo\n",
        "    'rf__min_samples_leaf': [1, 2, 4],  # Mínimo número de muestras requeridas en cada hoja\n",
        "    'rf__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases en la validación\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin SMOTE\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51eyENcyABxM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier  # Para Random Forest\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE  # Importar SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar Pipeline de imblearn\n",
        "\n",
        "\n",
        "print(\"TF-IDF RANDOM FOREST CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline con SMOTE para sobremuestreo, TF-IDF y Random Forest\n",
        "pipeline = ImbPipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicamos SMOTE\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Clasificación con Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Random Forest\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [100, 200, 500],  # Número de árboles en el bosque\n",
        "    'rf__max_depth': [None, 10, 20, 30],  # Profundidad máxima de los árboles\n",
        "    'rf__min_samples_split': [2, 5, 10],  # Mínimo número de muestras requeridas para dividir un nodo\n",
        "    'rf__min_samples_leaf': [1, 2, 4],  # Mínimo número de muestras requeridas en cada hoja\n",
        "    'rf__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases en la validación\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con SMOTE\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObtXygmIABxN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB  # Para Naive Bayes\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from imblearn.over_sampling import SMOTE  # Importar SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar Pipeline de imblearn\n",
        "\n",
        "\n",
        "print(\"TF-IDF NAIVE BAYES CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline con SMOTE para sobremuestreo, TF-IDF y Naive Bayes\n",
        "pipeline = ImbPipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicamos SMOTE\n",
        "    ('nb', MultinomialNB())  # Clasificación con Naive Bayes\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Naive Bayes\n",
        "param_grid = {\n",
        "    'nb__alpha': [0.1, 0.5, 1.0],  # Parámetro de suavización de Naive Bayes\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases en la validación\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con SMOTE\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CwBaOUlABxN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Cambiado a TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB  # Para Naive Bayes\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.pipeline import Pipeline  # Pipeline de sklearn\n",
        "\n",
        "\n",
        "print(\"TF-IDF NAIVE BAYES SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline con TF-IDF y Naive Bayes (sin SMOTE)\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Cambiado a TF-IDF\n",
        "    ('nb', MultinomialNB())  # Clasificación con Naive Bayes\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo Naive Bayes\n",
        "param_grid = {\n",
        "    'nb__alpha': [0.1, 0.5, 1.0],  # Parámetro de suavización de Naive Bayes\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases en la validación\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin SMOTE\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbsVfj0wABxN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"TF-IDF CatBoost CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización, SMOTE y clasificación usando CatBoost\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Usamos TF-IDF en lugar de CountVectorizer\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para balanceo de clases\n",
        "    ('catboost', CatBoostClassifier(learning_rate=0.1, iterations=200, depth=6, random_state=42, verbose=0))  # CatBoost\n",
        "])\n",
        "\n",
        "# Reducimos el rango de hiperparámetros\n",
        "param_grid = {\n",
        "    'catboost__learning_rate': [0.05, 0.1],  # Reducimos el rango de tasas de aprendizaje\n",
        "    'catboost__iterations': [100, 200],  # Menos iteraciones para acelerar\n",
        "    'catboost__depth': [6]  # Usamos un solo valor de profundidad para acortar\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Reducimos el número de splits a 3\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con menos combinaciones para acelerar\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8RuXNyIABxO",
        "outputId": "8f049337-4283-4c40-ad58-58fbda43f4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF CatBoost SIN SMOTE\n",
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "Mejores hiperparámetros: {'catboost__depth': 6, 'catboost__iterations': 100, 'catboost__learning_rate': 0.05} sin smote\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matthews Correlation Coefficient (MCC): Media = 0.0000, Desvío estándar = 0.0000\n",
            "F1-score: Media = 0.0000, Desvío estándar = 0.0000\n",
            "Recall: Media = 0.0000, Desvío estándar = 0.0000\n",
            "Precision: Media = 0.0000, Desvío estándar = 0.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "print(\"TF-IDF CatBoost SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando CatBoost\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),  # Usamos TF-IDF en lugar de CountVectorizer\n",
        "    ('catboost', CatBoostClassifier(learning_rate=0.1, iterations=200, depth=6, random_state=42, verbose=0))  # CatBoost\n",
        "])\n",
        "\n",
        "# Reducimos el rango de hiperparámetros\n",
        "param_grid = {\n",
        "    'catboost__learning_rate': [0.05, 0.1],  # Reducimos el rango de tasas de aprendizaje\n",
        "    'catboost__iterations': [100, 200],  # Menos iteraciones para acelerar\n",
        "    'catboost__depth': [6]  # Usamos un solo valor de profundidad para acortar\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Reducimos el número de splits a 3\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con menos combinaciones para acelerar\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6ZXkZDHABxP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Word2Vec LOGREG SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Regresión Logística\n",
        "pipeline = Pipeline([\n",
        "    ('word2vec', Word2VecTransformer(vector_size=100)),  # Word2Vec\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P90BeG-eABxR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Word2Vec LOGREG CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Construimos el pipeline con SMOTE y Regresión Logística\n",
        "pipeline = ImbPipeline([\n",
        "    ('word2vec', Word2VecTransformer(vector_size=100)),  # Word2Vec\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para balancear las clases\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ0emhvgABxS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "print(\"Word2Vec Naive Bayes CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras y toma el valor absoluto para evitar valores negativos\n",
        "                return np.abs(np.mean([model.wv[word] for word in tokens], axis=0))\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Creamos una lista de validación cruzada para usarla en el grid search\n",
        "def grid_search_pipeline(X, y):\n",
        "    # Aplicamos Word2Vec con el pipeline\n",
        "    word2vec_transformer = Word2VecTransformer(vector_size=100)\n",
        "    X_transformed = word2vec_transformer.fit_transform(X)\n",
        "\n",
        "    # Aplicamos SMOTE sobre los datos transformados\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_transformed, y)\n",
        "\n",
        "    # Configuramos Naive Bayes\n",
        "    nb = MultinomialNB()\n",
        "\n",
        "    # Realizamos la validación cruzada usando el clasificador Naive Bayes\n",
        "    scoring = {\n",
        "        'f1': make_scorer(f1_score),\n",
        "        'recall': make_scorer(recall_score),\n",
        "        'precision': make_scorer(precision_score),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Realizamos validación cruzada\n",
        "    final_scores = cross_validate(nb, X_resampled, y_resampled, cv=skf, scoring=scoring)\n",
        "\n",
        "    # Imprimir las métricas con su media y desvío estándar\n",
        "    print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "    print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "    print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "    print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n",
        "\n",
        "# Llamar a la función para hacer el GridSearch y la evaluación\n",
        "grid_search_pipeline(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqF_QJD7ABxT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "print(\"Word2Vec Naive Bayes CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras y toma el valor absoluto para evitar valores negativos\n",
        "                return np.abs(np.mean([model.wv[word] for word in tokens], axis=0))\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Creamos una lista de validación cruzada para usarla en el grid search\n",
        "def grid_search_pipeline(X, y):\n",
        "    # Aplicamos Word2Vec con el pipeline\n",
        "    word2vec_transformer = Word2VecTransformer(vector_size=100)\n",
        "    X_transformed = word2vec_transformer.fit_transform(X)\n",
        "\n",
        "    # Aplicamos SMOTE sobre los datos transformados\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_transformed, y)\n",
        "\n",
        "    # Configuramos Naive Bayes\n",
        "    nb = MultinomialNB()\n",
        "\n",
        "    # Realizamos la validación cruzada usando el clasificador Naive Bayes\n",
        "    scoring = {\n",
        "        'f1': make_scorer(f1_score),\n",
        "        'recall': make_scorer(recall_score),\n",
        "        'precision': make_scorer(precision_score),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Realizamos validación cruzada\n",
        "    final_scores = cross_validate(nb, X_resampled, y_resampled, cv=skf, scoring=scoring)\n",
        "\n",
        "    # Imprimir las métricas con su media y desvío estándar\n",
        "    print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "    print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "    print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "    print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n",
        "\n",
        "# Llamar a la función para hacer el GridSearch y la evaluación\n",
        "grid_search_pipeline(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flXid_eZABxa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Word2Vec Random Forest CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Construimos el pipeline con SMOTE y Random Forest\n",
        "pipeline = ImbPipeline([\n",
        "    ('word2vec', Word2VecTransformer(vector_size=100)),  # Word2Vec\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para balancear las clases\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Random Forest\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [100, 200, 300],  # Número de árboles en el bosque\n",
        "    'rf__max_depth': [None, 10, 20, 30],  # Profundidad máxima de los árboles\n",
        "    'rf__min_samples_split': [2, 5, 10],  # Número mínimo de muestras necesarias para dividir un nodo\n",
        "    'rf__min_samples_leaf': [1, 2, 4],  # Número mínimo de muestras necesarias en un nodo hoja\n",
        "    'rf__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH7NasntABxc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Word2Vec Naive Bayes SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras y toma el valor absoluto para evitar valores negativos\n",
        "                return np.abs(np.mean([model.wv[word] for word in tokens], axis=0))\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Creamos una lista de validación cruzada para usarla en el grid search\n",
        "def grid_search_pipeline(X, y):\n",
        "    # Aplicamos Word2Vec con el pipeline\n",
        "    word2vec_transformer = Word2VecTransformer(vector_size=100)\n",
        "    X_transformed = word2vec_transformer.fit_transform(X)\n",
        "\n",
        "    # Configuramos Naive Bayes\n",
        "    nb = MultinomialNB()\n",
        "\n",
        "    # Realizamos la validación cruzada usando el clasificador Naive Bayes\n",
        "    scoring = {\n",
        "        'f1': make_scorer(f1_score),\n",
        "        'recall': make_scorer(recall_score),\n",
        "        'precision': make_scorer(precision_score),\n",
        "        'mcc': make_scorer(matthews_corrcoef)\n",
        "    }\n",
        "\n",
        "    # Realizamos validación cruzada\n",
        "    final_scores = cross_validate(nb, X_transformed, y, cv=skf, scoring=scoring)\n",
        "\n",
        "    # Imprimir las métricas con su media y desvío estándar\n",
        "    print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "    print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "    print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "    print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n",
        "\n",
        "# Llamar a la función para hacer el GridSearch y la evaluación\n",
        "grid_search_pipeline(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7644g6JABxe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Word2Vec Random Forest SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Construimos el pipeline sin SMOTE y con Random Forest\n",
        "pipeline = Pipeline([\n",
        "    ('word2vec', Word2VecTransformer(vector_size=100)),  # Word2Vec\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Random Forest\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [100, 200, 300],  # Número de árboles en el bosque\n",
        "    'rf__max_depth': [None, 10, 20, 30],  # Profundidad máxima de los árboles\n",
        "    'rf__min_samples_split': [2, 5, 10],  # Número mínimo de muestras necesarias para dividir un nodo\n",
        "    'rf__min_samples_leaf': [1, 2, 4],  # Número mínimo de muestras necesarias en un nodo hoja\n",
        "    'rf__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nnm-vyLkABxf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Word2Vec CatBoost CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Construimos el pipeline para Word2Vec, SMOTE y clasificación usando CatBoost\n",
        "pipeline = Pipeline([\n",
        "    ('word2vec', Word2VecTransformer(vector_size=100)),  # Word2Vec\n",
        "    ('smote', SMOTE(random_state=42)),  # SMOTE para balanceo de clases\n",
        "    ('catboost', CatBoostClassifier(learning_rate=0.1, iterations=200, depth=6, random_state=42, verbose=0))  # CatBoost\n",
        "])\n",
        "\n",
        "# Reducimos el rango de hiperparámetros\n",
        "param_grid = {\n",
        "    'catboost__learning_rate': [0.05, 0.1],  # Reducimos el rango de tasas de aprendizaje\n",
        "    'catboost__iterations': [100, 200],  # Menos iteraciones para acelerar\n",
        "    'catboost__depth': [6]  # Usamos un solo valor de profundidad para acortar\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Reducimos el número de splits a 3\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con menos combinaciones para acelerar\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVMJDLZ-ABxg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from imblearn.pipeline import Pipeline\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "print(\"Word2Vec CatBoost SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo Word2Vec\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = Word2Vec(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_word2vec(tokens, model, vector_size):\n",
        "            # Elimina palabras desconocidas del modelo\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                # Promedia los vectores de las palabras\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        # Transformamos el conjunto de datos\n",
        "        return np.array([get_average_word2vec(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Construimos el pipeline para Word2Vec y clasificación usando CatBoost (sin SMOTE)\n",
        "pipeline = Pipeline([\n",
        "    ('word2vec', Word2VecTransformer(vector_size=100)),  # Word2Vec\n",
        "    ('catboost', CatBoostClassifier(learning_rate=0.1, iterations=200, depth=6, random_state=42, verbose=0))  # CatBoost\n",
        "])\n",
        "\n",
        "# Reducimos el rango de hiperparámetros\n",
        "param_grid = {\n",
        "    'catboost__learning_rate': [0.05, 0.1],  # Reducimos el rango de tasas de aprendizaje\n",
        "    'catboost__iterations': [100, 200],  # Menos iteraciones para acelerar\n",
        "    'catboost__depth': [6]  # Usamos un solo valor de profundidad para acortar\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Reducimos el número de splits a 3\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con menos combinaciones para acelerar\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZBP-R3lABxx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "print(\"FastText LOGREG SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Regresión Logística\n",
        "pipeline = Pipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X, y, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTe10p_zABxy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar Pipeline de imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "print(\"FastText LOGREG CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos antes de SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización, sobremuestreo con SMOTE y clasificación usando Regresión Logística\n",
        "pipeline = ImbPipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicar SMOTE solo en el entrenamiento\n",
        "    ('logreg', LogisticRegression(random_state=42, max_iter=1000))  # Regresión Logística\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Regresión Logística\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'logreg__solver': ['liblinear', 'lbfgs'],  # Algoritmos de optimización\n",
        "    'logreg__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GJejE8aABx0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar Pipeline de imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "print(\"FastText SVM CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos antes de SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización, sobremuestreo con SMOTE y clasificación usando SVM\n",
        "pipeline = ImbPipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicar SMOTE solo en el entrenamiento\n",
        "    ('svm', SVC(random_state=42))  # Clasificación usando SVM\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de SVM\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'svm__kernel': ['linear', 'rbf'],  # Tipos de kernel\n",
        "    'svm__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cKoLPtyABx2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar Pipeline de imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "print(\"FastText RANDOM FOREST CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos antes de aplicar SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización, sobremuestreo con SMOTE y clasificación usando Random Forest\n",
        "pipeline = ImbPipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicar SMOTE solo en el entrenamiento\n",
        "    ('rf', RandomForestClassifier(random_state=42))  # Clasificación usando Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Random Forest\n",
        "param_grid = {\n",
        "    'rf__n_estimators': [100, 200, 500],  # Número de árboles en el bosque\n",
        "    'rf__max_depth': [10, 20, None],  # Máxima profundidad del árbol\n",
        "    'rf__min_samples_split': [2, 5, 10],  # Mínimo número de muestras requeridas para dividir un nodo\n",
        "    'rf__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIEV7eMXABx4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar Pipeline de imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "print(\"FastText NAIVE BAYES CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos antes de aplicar SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización, sobremuestreo con SMOTE y clasificación usando Naive Bayes\n",
        "pipeline = ImbPipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('scaler', MinMaxScaler()),  # Escalar los datos a [0, 1] para Naive Bayes\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicar SMOTE solo en el entrenamiento\n",
        "    ('nb', MultinomialNB())  # Clasificación usando Naive Bayes\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Naive Bayes\n",
        "param_grid = {\n",
        "    'nb__alpha': [0.1, 0.5, 1.0],  # Suavizado de Laplace\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2JdCzMBABx8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline  # Usar Pipeline de imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "print(\"FastText CATBOOST CON SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos antes de aplicar SMOTE\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización, sobremuestreo con SMOTE y clasificación usando CatBoost\n",
        "pipeline = ImbPipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('smote', SMOTE(random_state=42)),  # Aplicar SMOTE solo en el entrenamiento\n",
        "    ('catboost', CatBoostClassifier(random_state=42, verbose=0))  # Clasificación usando CatBoost\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo CatBoost\n",
        "param_grid = {\n",
        "    'catboost__iterations': [100, 200],\n",
        "    'catboost__learning_rate': [0.01, 0.1],\n",
        "    'catboost__depth': [4, 6, 8],  # Profundidad de los árboles\n",
        "    'catboost__l2_leaf_reg': [1, 3, 5]  # Regularización L2\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} con smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4waSuvxeAByJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "print(\"FastText SVM SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando SVM\n",
        "pipeline = Pipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('svm', SVC(random_state=42))  # Clasificación usando SVM\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de SVM\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10, 100],  # Parámetro de regularización\n",
        "    'svm__kernel': ['linear', 'rbf'],  # Tipos de kernel\n",
        "    'svm__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Imprimimos los mejores hiperparámetros encontrados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_} sin smote\")\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stUEKSPUAByM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "print(\"FastText Naive Bayes SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Naive Bayes\n",
        "pipeline = Pipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('naive_bayes', GaussianNB())  # Clasificación usando Naive Bayes\n",
        "])\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada (sin hiperparámetros específicos en Naive Bayes)\n",
        "grid_search = GridSearchCV(pipeline, param_grid={}, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z41wCDSAByN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "print(\"FastText Random Forest SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando Random Forest\n",
        "pipeline = Pipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('random_forest', RandomForestClassifier(random_state=42))  # Clasificación usando Random Forest\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de Random Forest\n",
        "param_grid = {\n",
        "    'random_forest__n_estimators': [100, 200],  # Número de árboles en el bosque\n",
        "    'random_forest__max_depth': [None, 10, 20],  # Profundidad máxima de los árboles\n",
        "    'random_forest__min_samples_split': [2, 5],  # Número mínimo de muestras necesarias para dividir un nodo\n",
        "    'random_forest__min_samples_leaf': [1, 2],  # Número mínimo de muestras necesarias en una hoja\n",
        "    'random_forest__class_weight': [None, 'balanced']  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iybuD6KfAByP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, matthews_corrcoef\n",
        "from gensim.models import FastText\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "print(\"FastText CatBoost SIN SMOTE\")\n",
        "\n",
        "X = df['text_clean']  # Texto procesado\n",
        "y = df['label']  # Etiquetas de sesgo\n",
        "\n",
        "# Crea un transformador personalizado para convertir texto a vectores usando FastText\n",
        "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Tokenizamos el texto y entrenamos el modelo FastText\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=4)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Transforma el texto en vectores promediados de las palabras\n",
        "        def get_average_fasttext(tokens, model, vector_size):\n",
        "            tokens = [word for word in tokens if word in model.wv]\n",
        "            if len(tokens) == 0:\n",
        "                return np.zeros(vector_size)\n",
        "            else:\n",
        "                return np.mean([model.wv[word] for word in tokens], axis=0)\n",
        "\n",
        "        return np.array([get_average_fasttext(sentence.split(), self.model, self.vector_size) for sentence in X])\n",
        "\n",
        "# Dividimos los datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Construimos el pipeline para vectorización y clasificación usando CatBoost\n",
        "pipeline = Pipeline([\n",
        "    ('fasttext', FastTextTransformer(vector_size=100)),  # FastText embeddings\n",
        "    ('catboost', CatBoostClassifier(verbose=0, random_state=42))  # Clasificación usando CatBoost\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros que queremos ajustar en el modelo de CatBoost\n",
        "param_grid = {\n",
        "    'catboost__iterations': [100, 200],  # Número de iteraciones o árboles\n",
        "    'catboost__depth': [6, 8, 10],  # Profundidad máxima del árbol\n",
        "    'catboost__learning_rate': [0.01, 0.1],  # Tasa de aprendizaje\n",
        "    'catboost__l2_leaf_reg': [1, 3, 5],  # Regularización L2\n",
        "    'catboost__border_count': [32, 64],  # Número de puntos de división en las características continuas\n",
        "    'catboost__class_weights': [None, {0: 1, 1: 2}]  # Ajuste de peso de clases\n",
        "}\n",
        "\n",
        "# Definir las métricas que queremos evaluar\n",
        "scoring = {\n",
        "    'f1': make_scorer(f1_score),\n",
        "    'recall': make_scorer(recall_score),\n",
        "    'precision': make_scorer(precision_score),\n",
        "    'mcc': make_scorer(matthews_corrcoef)  # Matthew's Correlation Coefficient\n",
        "}\n",
        "\n",
        "# Usamos StratifiedKFold para manejar el desbalance de clases\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Configuramos GridSearchCV con validación cruzada\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='f1', refit=True, verbose=1)\n",
        "\n",
        "# Entrenamos el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mejor estimador\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Realizamos validación cruzada para evaluación final con múltiples métricas\n",
        "final_scores = cross_validate(best_model, X_test, y_test, cv=skf, scoring=scoring)\n",
        "\n",
        "# Imprimir las métricas con su media y desvío estándar\n",
        "print(f\"Matthews Correlation Coefficient (MCC): Media = {final_scores['test_mcc'].mean():.4f}, Desvío estándar = {final_scores['test_mcc'].std():.4f}\")\n",
        "print(f\"F1-score: Media = {final_scores['test_f1'].mean():.4f}, Desvío estándar = {final_scores['test_f1'].std():.4f}\")\n",
        "print(f\"Recall: Media = {final_scores['test_recall'].mean():.4f}, Desvío estándar = {final_scores['test_recall'].std():.4f}\")\n",
        "print(f\"Precision: Media = {final_scores['test_precision'].mean():.4f}, Desvío estándar = {final_scores['test_precision'].std():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}